{
  "hash": "8565cbe6a53f7166decc3735846b0f7f",
  "result": {
    "markdown": "---\ntitle: \"Scraping paginated HTML sites in R\"\nauthor: Jonathan\ndate: 09-19-2022\ndescription: \"Showcasing web scraping in R with a focus on historical Swedish data\"\nimage: \"https://raw.githubusercontent.com/j-jayes/screencasts/main/images/tax.jpg\"\nformat: \n  html:\n    code-link: true\n    code-overflow: wrap\ndraft: true\nexecute:\n  freeze: true\n  eval: false\n---\n\n## Purpose\n\nScrape the website of the Stockholm city archive.\n\n## Structure of the screencast\n\n### Look at the webpage\n\n- View page source\n\n### Check permission\n\n- Robots.txt file\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(robotstxt)\nget_robotstxt(\"https://sok.stadsarkivet.stockholm.se/\")\n```\n:::\n\n\n\n### Function to get the table elements\n\n- Using `rvest` to get the table elements\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(tidyverse)\n\nurl <- \"https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1800-1884/Sok?sidindex=0&artal=1800\"\n\nget_table_data <- function(url) {\n  message(\"Getting data from \", url)\n  html <- read_html(url)\n\n  table <- html %>%\n    html_nodes(\".table\") %>%\n    html_table()\n\n  table <- table[[1]]\n\n  table <- table %>%\n    filter(between(row_number(), 4, 13)) %>%\n    select(1:7) %>%\n    rename(\n      year = X1,\n      district = X2,\n      index = X3,\n      surname = X4,\n      first_name = X5,\n      title = X6,\n      other = X7\n    )\n  \n  table\n}\n\n# test\nget_table_data(url)\n```\n:::\n\n\n\n### Get the number of pages on which there is data\n\n- Write a neat little function for this\n\n::: {.cell}\n\n```{.r .cell-code}\nget_n_pages <- function(url) {\n  html <- read_html(url)\n  message(\"Getting the number of pages from \", url)\n\n  n_res <- html %>%\n    html_nodes(\".antaltraffar-div\") %>%\n    html_text() %>%\n    str_squish() %>%\n    str_extract(\"[0-9].*\") %>%\n    str_remove(., \" \") %>%\n    parse_number()\n\n  n_pages <- ceiling(n_res / 10)\n\n  n_pages\n}\n\n# test\nget_n_pages(url)\n```\n:::\n\n\n\n### Create a workflow to go through\n\n- Include parameters that we can change\n\n::: {.cell}\n\n```{.r .cell-code}\nyear_search <- 1800\nbase_url <- paste0(\"https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1800-1884/Sok?sidindex=0&artal=\", year_search)\nstart_page <- 1\nfinal_page <- get_n_pages(base_url)\n\nlist_of_pages <- tibble(\n  url = paste0(\"https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1800-1884/Sok?sidindex=\", start_page:final_page, \"&artal=\", year_search),\n  page = start_page:final_page\n)\n```\n:::\n\n\n\n### Iterate through the list of pages\n\n- Using the map function\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_out <- list_of_pages %>% \n  head(10) %>% \n  mutate(data = map(url, possibly(get_table_data, \"failed\"))) %>% \n  select(!url)\n\ndf_out <- df_out %>% \n  unnest(data)\n```\n:::\n\n\n\n### Save the data to a csv file\n\n- Some data management\n\n::: {.cell}\n\n```{.r .cell-code}\ntime <- Sys.time()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}