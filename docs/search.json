[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kathy’s coding club",
    "section": "",
    "text": "This is a website that collects the contents of Kathy’s coding club"
  },
  {
    "objectID": "code/Chocolate_bar_ratings.html",
    "href": "code/Chocolate_bar_ratings.html",
    "title": "Chocolate bar ratings",
    "section": "",
    "text": "We’re going to be diving into the reviews of chocolate bars from the Manhattan Chocolate Society, who have rated over 2500 chocolate bars from around the world. We are going to be trying to answer a few questions about their ratings.\nThe data is accessible from the R for Data Science Tidy Tuesday repository on Github.\nIf you have any questions about the different commands that we use and why - have a look at this post on my website. You can click on any of the commands and read the documentation to get a better understanding of how they are used, or which package they are from.\nThen let’s get to it - the first step is to open a fresh Rmarkdown document - this allows you to keep human readable notes interspersed with your code that you can return to later, and export the prose and code in a number of different ways to share it, with colleagues or on the internet.\nThe next thing which I think is worth doing is to change the default IDE theme in Rstudio to darkmode. This is easier on your eyes and makes you look professional. You can find the Rstudio appearance settings under tools, and then global options. I like the Cobalt theme, but there are many to choose from.\nOne other thing I would suggest is the use of rainbow parentheses - these make it easy to see where you’re missing a bracket. You can enable them under the code tab in Global options.\nGreat - then let’s get to it!\nFirst we will call the tidyverse meta package - a cohesive group of packages that make working with messy data very easy - originally coined by Hadley Wickham but improved upon by a great many contributors. I’ll leave a link in the description to more info about this.\nThen we read in the data from the R4DS Tidy Tuesday Repo.\n\nlibrary(tidyverse)\n\ndf <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv')\n\ntheme_set(theme_light())\n\nExploratory data analysis\nFirst we can have a look at the data we have read in - This gives us a tibble (similar to a dataframe) with 10 columns (3 numeric and 7 character) and 2,530 individual reviews.\n\ndf %>% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ncocoa_percent\n0\n1.00\n3\n6\n0\n46\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nref\n0\n1\n1429.80\n757.65\n5\n802\n1454.00\n2079.0\n2712\n▆▇▇▇▇\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006\n2012\n2015.00\n2018.0\n2021\n▃▅▇▆▅\n\n\nrating\n0\n1\n3.20\n0.45\n1\n3\n3.25\n3.5\n4\n▁▁▅▇▇\n\n\n\n\n\nNext we can look at some of the common countries in which the beans are processed into bars, as well as where the beans originate.\n\ndf %>% \n  count(company_location, sort = T)\n\n# A tibble: 67 x 2\n   company_location     n\n   <chr>            <int>\n 1 U.S.A.            1136\n 2 Canada             177\n 3 France             176\n 4 U.K.               133\n 5 Italy               78\n 6 Belgium             63\n 7 Ecuador             58\n 8 Australia           53\n 9 Switzerland         44\n10 Germany             42\n# ... with 57 more rows\n\ndf %>% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 x 2\n   country_of_bean_origin     n\n   <chr>                  <int>\n 1 Venezuela                253\n 2 Peru                     244\n 3 Dominican Republic       226\n 4 Ecuador                  219\n 5 Madagascar               177\n 6 Blend                    156\n 7 Nicaragua                100\n 8 Bolivia                   80\n 9 Colombia                  79\n10 Tanzania                  79\n# ... with 52 more rows\n\n\nLooking at the bar ratings, we can see a distribution that is centred just above 3 - this is in line with what we saw from the skim of the dataset above - a mean rating of 3.2\n\ndf %>%\n  ggplot(aes(rating)) +\n  geom_histogram(binwidth = .25, fill = \"midnightblue\") +\n  labs(x = \"Chocolate bar rating\",\n       y = \"Number of bars\")\n\n\n\n\nInitial questions\n\nHave ratings been going up over time?\n\nThe first simple plot we can make to examine this question is a boxplot - we can use ggplot to create a boxplot, mapping review date to the x axis, rating to the y axis, and we include the command group = review_date.\n\ndf %>% \n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_boxplot()\n\n\n\n\nGreat - we can see that the median ranges between 3 and 3.25, increasing in 2010. We can also see that the bottom of the distribution has moved up over time. In other words, there are fewer low scoring chocolate bars over time - potentially indicating an increase in quality of the bars or greater leniency on the part of the reviewers.\nWe can visualize this distribution with the help of another package called ggridges.\nI’ll build up this visualization in stages:\nWe’ll start by mapping the ratings onto the x-axis, the year of the review onto the y-axis (as a factor rather than as a continuous variable).\n\nlibrary(ggridges)\n\ndf %>% \n  ggplot(aes(rating, y = factor(review_date))) +\n  geom_density_ridges()\n\n\n\n\nGreat - we can see that the pattern we saw earlier is clear - as time increases up the y-axis, the share of bars receiving reviews below three decreases.\nNext we can add in a colour scale for the fill:\n\ndf %>% \n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n\n\nThe viridis colour scales are nice for two reasons - they’re discernible to people with most forms of colour blindness and print well if you only use black and white. The magma option gives a nice fade from purple to yellow.\nNext we can move the colourbar in the legend to the bottom and increase it in size, and add some labels.\n\ndf %>%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\nWhat can we learn about the number of ingredients and cocoa percentage?\nSelecting the ingredients column, we see that it has the number of ingredients and each ingredient listed after a dash and separated by commas.\n\ndf %>% \n  select(ingredients)\n\n# A tibble: 2,530 x 1\n   ingredients\n   <chr>      \n 1 3- B,S,C   \n 2 3- B,S,C   \n 3 3- B,S,C   \n 4 3- B,S,C   \n 5 3- B,S,C   \n 6 3- B,S,C   \n 7 3- B,S,C   \n 8 4- B,S,C,L \n 9 4- B,S,C,L \n10 4- B,S,C,L \n# ... with 2,520 more rows\n\n\nThe chocolate bar ingredients are:\n\nstr <- \"B = Beans, S = Sugar, S* = Sweetener other than white cane or beet sugar, C = Cocoa Butter, V = Vanilla, L = Lecithin, Sa = Salt\"\n\nstr <- str %>% \n  as_tibble() %>% \n  separate_rows(value, sep = \",\") %>% \n  separate(value, c(\"key\", \"value\"), \"=\") %>% \n  mutate(across(c(key, value), str_squish))\n\nknitr::kable(str)\n\n\n\nkey\nvalue\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\n\ndf <- df %>% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nWe can use the separate function from dplyr to split the ingredients column into two columns, based on the dash which splits the two components.\nThe separate function takes three inputs: 1. the column which you want to split, 2. a character vector of new column names, and 3. finally the regex which separates the columns.\n\ndf %>% \n  separate(ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"), \n           sep = \"-\") %>% \n  select(n_ingredients, ingredients) %>%\n  mutate(n_ingredients = parse_number(n_ingredients))\n\n# A tibble: 2,530 x 2\n   n_ingredients ingredients                         \n           <dbl> <chr>                               \n 1             3 \" beans,sugar,cocoa_butter\"         \n 2             3 \" beans,sugar,cocoa_butter\"         \n 3             3 \" beans,sugar,cocoa_butter\"         \n 4             3 \" beans,sugar,cocoa_butter\"         \n 5             3 \" beans,sugar,cocoa_butter\"         \n 6             3 \" beans,sugar,cocoa_butter\"         \n 7             3 \" beans,sugar,cocoa_butter\"         \n 8             4 \" beans,sugar,cocoa_butter,lecithin\"\n 9             4 \" beans,sugar,cocoa_butter,lecithin\"\n10             4 \" beans,sugar,cocoa_butter,lecithin\"\n# ... with 2,520 more rows\n\n\nWith the number of ingredients in it’s own column now we can ask what share of the chocolate bars\n\n# jpeg(filename = \"figures/Chocolate_bar_ratings.jpeg\",\n#      height = 6,\n#      width = 8,\n#      units = \"in\",\n#      res = 1000)\n\ndf %>% \n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>% \n  mutate(across(c(n_ingredients, cocoa_percent), parse_number),\n         cocoa_percent = cocoa_percent - cocoa_percent %% 5) %>% \n  count(cocoa_percent, n_ingredients) %>% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars\")\n\n\n\n# dev.off()\n\nWhat is the correlation between ingredients that are used together?\n\ndf <- df %>%\n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>%\n  separate_rows(ingredients, sep = \",\") %>%\n  filter(!is.na(ingredients)) %>%\n  pivot_wider(names_from = ingredients, values_from = ingredients) %>%\n  mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nWhat do we know about the countries of origin?\n\ndf %>% \n  add_count(country_of_bean_origin) %>%\n  # only include countries with more than 60 bars\n  filter(n > 60) %>% \n  group_by(country_of_bean_origin) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %>% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\nWhat are some fun variables??\n\ndf %>% \n  select(most_memorable_characteristics, rating) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(across(most_memorable_characteristics, str_squish)) %>% \n  add_count(most_memorable_characteristics) %>% \n  filter(n > 15) %>% \n  group_by(most_memorable_characteristics) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  ungroup() %>% \n  arrange(mean_rating) %>% \n  slice(1:10, 69:78)\n\n# A tibble: 20 x 2\n   most_memorable_characteristics mean_rating\n   <chr>                                <dbl>\n 1 chemical                              2.5 \n 2 medicinal                             2.55\n 3 off notes                             2.58\n 4 burnt                                 2.72\n 5 rubber                                2.73\n 6 pungent                               2.73\n 7 metallic                              2.78\n 8 off                                   2.78\n 9 bitter                                2.79\n10 rubbery                               2.83\n11 cocoa                                 3.39\n12 melon                                 3.40\n13 nuts                                  3.40\n14 raisins                               3.41\n15 honey                                 3.42\n16 dried fruit                           3.44\n17 rich cocoa                            3.44\n18 orange                                3.45\n19 rich                                  3.46\n20 strawberry                            3.46\n\n\nWord model\n\ndf_characteristics <- df %>% \n  select(c(most_memorable_characteristics, rating)) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\ndf_characteristics %>% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 x 2\n   most_memorable_characteristics     n\n   <chr>                          <int>\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# ... with 938 more rows\n\n\n\ndf_characteristics %>% \n  group_by(most_memorable_characteristics) %>% \n  add_count() %>% \n  filter(n > 3) %>% \n  mutate(avg_rating = mean(rating)) %>% \n  ungroup() %>% \n  distinct(most_memorable_characteristics, avg_rating) %>% \n  slice_max(avg_rating, n = 12, with_ties = F) %>% \n    mutate(avg_rating = round(avg_rating, 2)) %>% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds <- vfold_cv(df_characteristics)\n\nglmnet_recipe <- \n  recipe(formula = rating ~ ., data = df_characteristics) %>% \n  step_tokenize(most_memorable_characteristics) %>% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% \n  step_tf(most_memorable_characteristics) %>% \n  step_normalize(all_predictors(), -all_nominal())\n\nglmnet_recipe %>% prep() %>% juice()\n\n# A tibble: 6,839 x 101\n   rating tf_most_memorable_~ tf_most_memorab~ tf_most_memorab~ tf_most_memorab~\n    <dbl>               <dbl>            <dbl>            <dbl>            <dbl>\n 1   3.25             -0.0767          -0.0630          -0.0805          -0.0528\n 2   3.25             -0.0767          -0.0630          -0.0805          -0.0528\n 3   3.25             -0.0767          -0.0630          -0.0805          -0.0528\n 4   3.5              -0.0767          -0.0630          -0.0805          -0.0528\n 5   3.5              -0.0767          -0.0630          -0.0805          -0.0528\n 6   3.5              -0.0767          -0.0630          -0.0805          -0.0528\n 7   3.75             -0.0767          -0.0630          -0.0805          -0.0528\n 8   3.75             -0.0767          -0.0630          -0.0805          -0.0528\n 9   3.75             -0.0767          -0.0630          -0.0805          -0.0528\n10   3                -0.0767          -0.0630          -0.0805          -0.0528\n# ... with 6,829 more rows, and 96 more variables:\n#   tf_most_memorable_characteristics_banana <dbl>,\n#   tf_most_memorable_characteristics_base <dbl>,\n#   tf_most_memorable_characteristics_basic <dbl>,\n#   tf_most_memorable_characteristics_berry <dbl>,\n#   tf_most_memorable_characteristics_bitter <dbl>,\n#   tf_most_memorable_characteristics_black <dbl>, ...\n\n\n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\nglmnet_tune %>% \n  autoplot()\n\n\n\n\n\nglmnet_model_final <- finalize_workflow(glmnet_workflow, glmnet_tune %>% \n  select_best())\n\nfinal_fit <- glmnet_model_final %>% \n  fit(df_characteristics)\n\n\nfinal_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %>%\n  mutate(sign = estimate > 0) %>%\n  group_by(sign) %>%\n  mutate(estimate = abs(estimate)) %>% \n  slice_max(estimate, n = 12) %>%\n  ungroup() %>%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %>% \n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")"
  },
  {
    "objectID": "code/Show_your_stripes.html",
    "href": "code/Show_your_stripes.html",
    "title": "Recreating Ed Hawkins’ Show Your Stripes graphic",
    "section": "",
    "text": "Recreate the show your stripes chart - sounds kinda fun and a nice place to showcase some tricks\nThe chart is shown at:\n\nknitr::include_url(\"https://showyourstripes.info/\")\n\n\n\n\nFrom the Met office\nThe Met Office has a number of different series on their website here: https://www.metoffice.gov.uk/hadobs/hadcrut5/data/current/download.html\n“Time series are presented as temperature anomalies (deg C) relative to 1961-1990.”\nMonthly data?\n\ndf <- read_csv(\"https://www.metoffice.gov.uk/hadobs/hadcrut5/data/current/analysis/diagnostics/HadCRUT.5.0.1.0.analysis.summary_series.global.monthly.csv\")\n\nRows: 2066 Columns: 4\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Time\ndbl (3): Anomaly (deg C), Lower confidence limit (2.5%), Upper confidence li...\n\n\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf <- df %>% \n  janitor::clean_names()\n\n\nLet’s begin by looking at the dataset with the skimr package:\n\ndf %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2066\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ntime\n0\n1\n7\n7\n0\n2066\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nanomaly_deg_c\n0\n1\n-0.08\n0.38\n-1.04\n-0.35\n-0.16\n0.11\n1.22\n▁▇▅▂▁\n\n\nlower_confidence_limit_2_5_percent\n0\n1\n-0.22\n0.44\n-1.22\n-0.55\n-0.31\n0.00\n1.18\n▁▇▅▂▁\n\n\nupper_confidence_limit_97_5_percent\n0\n1\n0.06\n0.34\n-0.87\n-0.17\n0.00\n0.22\n1.26\n▁▇▆▂▁\n\n\n\n\n\n\ndf %>% \n  ggplot(aes(time, anomaly_deg_c)) +\n  geom_point()\n\n\n\n\nThe x-axis is a bit of a mess!\nLet’s rather try to change these to a date with the help of the lubridate package.\n\ndf <- df %>% \n  mutate(time = lubridate::ymd(paste0(time, \"-01\")))\n\ndf %>%\n  ggplot(aes(time, anomaly_deg_c)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0, lty = 2)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWow! That’s way better - and now we have\n\n\ndf %>% \n  filter(str_detect(time, \"\\\\d\\\\d\\\\d\\\\d-01.*\")) %>% \n  ggplot(aes(x = time, ymin = lower_confidence_limit_2_5_percent, ymax = upper_confidence_limit_97_5_percent)) +\n  geom_ribbon(alpha = .5) +\n  geom_line(aes(x = time, y = anomaly_deg_c))\n\n\n\n\nWhat about averaging across the year? We can make use of the lubridate package again! This time we use the year function to get the year (as a number) out of the time column.\n\ndf %>% \n  mutate(year = lubridate::year(time)) %>% \n  group_by(year) %>% \n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>% \n  ungroup() %>% \n  distinct(year, .keep_all = T) %>% \n  ggplot(aes(x = time, ymin = lower_confidence_limit_2_5_percent, ymax = upper_confidence_limit_97_5_percent)) +\n  geom_ribbon(fill = \"grey70\") +\n  geom_line(aes(x = time, y = anomaly_deg_c))\n\n\n\n\nWe can see that as time goes on, we get better at measuring things with a lower margin of error.\n\n\ndf %>% \n  ggplot(aes(time, y = 1, fill = anomaly_deg_c)) + \n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  labs(x = \"Year\",\n       y = NULL,\n       caption = \"Inspired by Ed Hawkins\\nData from the Met Office\")\n\n\n\n\n\n# jpeg(filename = \"figures/Show_your_stripes.jpeg\",\n#      height = 6,\n#      width = 8,\n#      units = \"in\",\n#      res = 1000)\n\ndf %>%\n  mutate(year = lubridate::year(time)) %>%\n  group_by(year) %>%\n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>%\n  ungroup() %>%\n  distinct(year, .keep_all = T) %>%\n  ggplot(aes(time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspired by Ed Hawkins\\nData from the Met Office\"\n  )\n\n\n\n# dev.off()\n\n\n\ndf %>%\n  ggplot(aes(time, anomaly_deg_c, fill = anomaly_deg_c)) +\n  geom_col() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  geom_hline(yintercept = 0, lty = 2) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(barwidth = 25, barheight = 1, title.position = \"top\", title.hjust = 0.5))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Screencasts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nAn exploration of chocolte bar ratings\n\n\n\nJonathan\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ggplot to recreate a classic climate change visualization.\n\n\n\nJonathan\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "exams/econ31_4170_2020h.html",
    "href": "exams/econ31_4170_2020h.html",
    "title": "Data science for economists exams",
    "section": "",
    "text": "The university of Oslo has produced a course called Data Science for Economists at the bachelor level. I want to complete the exams as a resource for Kathy’s coding club, and because it looks fun!\n\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.1.2\n\ntheme_set(theme_light())\n\ninclude_graphics(here::here(\"exams\", \"econ31_4170_2020h.pdf\"))\n\n\n\n\n\n\n\nmobility <- read.csv(here::here(\"exams\", \"4170_vedlegg_2020_no_region_mobility_report.csv\")) %>% \n  as_tibble()\n\nmobility %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n34217\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n2\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncountry_region_code\n0\n1\n2\n2\n0\n1\n0\n\n\ncountry_region\n0\n1\n6\n6\n0\n1\n0\n\n\nsub_region_1\n0\n1\n0\n23\n277\n12\n0\n\n\nsub_region_2\n0\n1\n0\n28\n3324\n172\n0\n\n\niso_3166_2_code\n0\n1\n0\n5\n31170\n12\n0\n\n\ndate\n0\n1\n10\n10\n0\n277\n0\n\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\nmetro_area\n34217\n0\nNaN\n:\n\n\ncensus_fips_code\n34217\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nretail_and_recreation_percent_change_from_baseline\n21114\n0.38\n-5.79\n23.94\n-96\n-16\n-3\n7.0\n108\n▁▃▇▁▁\n\n\ngrocery_and_pharmacy_percent_change_from_baseline\n21786\n0.36\n2.24\n20.78\n-95\n-5\n4\n12.0\n104\n▁▁▇▁▁\n\n\nparks_percent_change_from_baseline\n29782\n0.13\n54.38\n75.57\n-80\n3\n37\n89.5\n616\n▇▃▁▁▁\n\n\ntransit_stations_percent_change_from_baseline\n18798\n0.45\n-24.98\n27.81\n-92\n-45\n-28\n-8.0\n135\n▃▇▂▁▁\n\n\nworkplaces_percent_change_from_baseline\n1632\n0.95\n-27.89\n19.68\n-92\n-42\n-24\n-13.0\n30\n▁▅▇▇▁\n\n\nresidential_percent_change_from_baseline\n22596\n0.34\n5.36\n5.72\n-11\n2\n4\n8.0\n33\n▁▇▃▁▁\n\n\n\n\nmobility %>% count() -> n_obs\n\nThere are 34217 observations.\n\n# transform into lubridate year month day format.\nmobility <- mobility %>% \n  mutate(date = lubridate::ymd(date))\n\nskimmed <- mobility %>% \n  skimr::skim()\n\nskimmed$Date.min %>% as_tibble() %>% filter(!is.na(value)) -> date_min\nskimmed$Date.max %>% as_tibble() %>% filter(!is.na(value)) -> date_max\n\nThe dates range from 2020-02-15 and 2020-11-17.\n\n\nmobility %>% count(sub_region_1, sort = T)\n\n# A tibble: 12 x 2\n   sub_region_1                  n\n   <chr>                     <int>\n 1 \"Viken\"                    7482\n 2 \"Vestland\"                 3776\n 3 \"Innlandet\"                3520\n 4 \"TrÃ¸ndelag\"               3277\n 5 \"Rogaland\"                 3227\n 6 \"Vestfold og Telemark\"     3061\n 7 \"MÃ¸re og Romsdal\"         2987\n 8 \"Nordland\"                 2250\n 9 \"Agder\"                    2242\n10 \"Troms og Finnmark fylke\"  1841\n11 \"\"                          277\n12 \"Oslo\"                      277\n\nmobility %>% count(sub_region_2, sort = T)\n\n# A tibble: 172 x 2\n   sub_region_2                    n\n   <chr>                       <int>\n 1 \"\"                           3324\n 2 \"Bergen Municipality\"         277\n 3 \"Trondheim Municipality\"      277\n 4 \"Ullensaker Municipality\"     277\n 5 \"Stavanger Municipality\"      275\n 6 \"Drammen Municipality\"        274\n 7 \"Kristiansand Municipality\"   274\n 8 \"LillestrÃ¸m\"                 273\n 9 \"TromsÃ¸ Municipality\"        273\n10 \"Ã…lesund Municipality\"       271\n# ... with 162 more rows\n\n\nUTF-8 can accommodate Norwegian characters like Ã.\n\n\noslo <- mobility %>% \n  filter(sub_region_1 == \"Oslo\")\n\noslo %>% \n  ggplot(aes(date, residential_percent_change_from_baseline)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nIn Oslo, it looks like there was a big spike in March and a decline in movement over the summer, before an increase again towards Christmas.\nThere appear to be some day of the week effects also!\n\n\noslo <- oslo %>%\n  mutate(\n    weekday = lubridate::wday(date, label = T),\n    weekend = case_when(\n      weekday %in% c(\"Sat\", \"Sun\") ~ 1,\n      TRUE ~ 0\n    ),\n    weekend = factor(weekend)\n  )\n\n\nplot_oslo <- function(var) {\n  oslo %>%\n    ggplot(aes(date, residential_percent_change_from_baseline, colour = {{ var }})) +\n    geom_point() +\n    geom_smooth(se = F)\n}\n\nplot_oslo(weekday) +\n  labs(colour = \"Weekday\")\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\nplot_oslo(weekend) +\n  labs(colour = \"Is it a weekend?\")\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n\nNote that they store aggregate data in a strange way - if sub_region_2 is an empty string but sub_region_1 names a county then this is aggregate data at the county level.\n\nmobility %>%\n  filter(sub_region_2 == \"\",\n         sub_region_1 != \"\") %>% \n  mutate(\n    weekday = lubridate::wday(date, label = T),\n    weekend = case_when(\n      weekday %in% c(\"Sat\", \"Sun\") ~ \"Weekend\",\n      TRUE ~ \"Weekday\"\n    )\n  ) %>% \n  ggplot(aes(date, residential_percent_change_from_baseline, colour = sub_region_1)) +\n  geom_line() +\n  # geom_smooth(se = F) +\n  facet_wrap(~ weekend, nrow = 2)\n\n\n\n\nThree weekday spikes are between April and June?\n\nIs this correct? Or should I be looking at squared differences?\n\nmobility %>%\n  filter(\n    sub_region_2 == \"\",\n    sub_region_1 != \"\",\n    between(date, lubridate::ymd(\"2020-03-12\"), lubridate::ymd(\"2020-10-31\"))\n  ) %>% \n  group_by(sub_region_1) %>% \n  summarise(mean_response = mean(residential_percent_change_from_baseline, na.rm = T),\n            median_response = median(residential_percent_change_from_baseline, na.rm = T)) %>% \n  ungroup() %>% \n  mutate(sub_region_1 = fct_reorder(sub_region_1, mean_response)) %>% \n  pivot_longer(-sub_region_1) %>% \n  mutate(value = round(value, 2)) %>% \n  ggplot(aes(value, sub_region_1, fill = name)) +\n  geom_col() +\n  geom_text(aes(label = value), hjust = 1.2) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  facet_wrap(~ name) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Summary of residential percent change from baseline\")\n\n\n\n\n\n\nnational <- mobility %>% \n  filter(sub_region_1 == \"\")\n\noslo_compare <- national %>% \n  select(residential_percent_change_from_baseline, date) %>% \n  rename(national = residential_percent_change_from_baseline) %>% \n  bind_cols(oslo %>% select(residential_percent_change_from_baseline) %>% \n              rename(oslo = residential_percent_change_from_baseline))\n\noslo_compare %>% \n  pivot_longer(-date) %>% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line()\n\n\n\noslo_compare %>% \n  mutate(week = lubridate::week(date)) %>% \n  group_by(week) %>% \n  mutate(across(c(national, oslo), mean)) %>% \n  ungroup() %>% \n  distinct(national, oslo, week) %>% \n  mutate(oslo_national = oslo / national) %>% \n  ggplot(aes(week, oslo_national)) +\n  geom_line()\n\n\n\n\n???\n\n\n\ndf <- tibble(x = 1:10,\n             y = 1:10)\n\ngamma = .5\ntheta = .3\nphi = 2\n\ndf %>% \n  mutate(comp_1 = ((x^(1-gamma)/1-gamma)),\n         comp_2 = phi*((y^(1-theta)/1-theta)),\n         utility = comp_1 + comp_2) %>% \n  pivot_longer(-c(x, y)) %>% \n  ggplot(aes(x, value, colour = name)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\nAxis is not from zero! Sneaky!\n\ndf <- tibble(chrome = 29.619,\n             edge = 31.786,\n             firefox = 26.876)\n\ndf %>% \n  pivot_longer(cols = everything()) %>% \n  mutate(name = fct_reorder(name, value)) %>% \n  ggplot(aes(value, name, fill = name)) +\n  geom_col(show.legend = F) +\n  labs(x = \"Speed score\",\n       y = NULL)\n\n\n\n\n\nSimilar!\n\ndf <- tibble(Latvia = 5.5,\n             Australia = 5.4,\n             Scotland = 5.4,\n             Peru = 5.4,\n             `South Africa` = 5.2,\n             India = 5)\n\ndf %>% \n  pivot_longer(cols = everything()) %>% \n  mutate(name = fct_reorder(name, value)) %>% \n  ggplot(aes(value, name, fill = name)) +\n  geom_col(show.legend = F) +\n  labs(x = \"Average female height\",\n       y = NULL)"
  }
]