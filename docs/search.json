[
  {
    "objectID": "code/Chocolate_bar_ratings.html",
    "href": "code/Chocolate_bar_ratings.html",
    "title": "Chocolate bar ratings",
    "section": "",
    "text": "We’re going to be diving into the reviews of chocolate bars from the Manhattan Chocolate Society, who have rated over 2500 chocolate bars from around the world. We are going to be trying to answer a few questions about their ratings.\nThe data is accessible from the R for Data Science Tidy Tuesday repository on Github.\nIf you have any questions about the different commands that we use and why - have a look at this post on my website. You can click on any of the commands and read the documentation to get a better understanding of how they are used, or which package they are from.\nThen let’s get to it - the first step is to open a fresh Rmarkdown document - this allows you to keep human readable notes interspersed with your code that you can return to later, and export the prose and code in a number of different ways to share it, with colleagues or on the internet.\nThe next thing which I think is worth doing is to change the default IDE theme in Rstudio to darkmode. This is easier on your eyes and makes you look professional. You can find the Rstudio appearance settings under tools, and then global options. I like the Cobalt theme, but there are many to choose from.\nOne other thing I would suggest is the use of rainbow parentheses - these make it easy to see where you’re missing a bracket. You can enable them under the code tab in Global options.\nGreat - then let’s get to it!\nFirst we will call the tidyverse meta package - a cohesive group of packages that make working with messy data very easy - originally coined by Hadley Wickham but improved upon by a great many contributors. I’ll leave a link in the description to more info about this.\nThen we read in the data from the R4DS Tidy Tuesday Repo.\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\ndf <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv')\n\nRows: 2530 Columns: 10\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (7): company_manufacturer, company_location, country_of_bean_origin, spe...\ndbl (3): ref, review_date, rating\n\n\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntheme_set(theme_light())\n\nEDA\nFirst we can have a look at the data we have read in - This gives us a tibble (similar to a dataframe) with 10 columns (3 numeric and 7 character) and 2,530 individual reviews.\n\ndf %>% view()\n\ndf %>% \n  count(company_location, sort = T)\n\n# A tibble: 67 x 2\n   company_location     n\n   <chr>            <int>\n 1 U.S.A.            1136\n 2 Canada             177\n 3 France             176\n 4 U.K.               133\n 5 Italy               78\n 6 Belgium             63\n 7 Ecuador             58\n 8 Australia           53\n 9 Switzerland         44\n10 Germany             42\n# ... with 57 more rows\n\ndf %>% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 x 2\n   country_of_bean_origin     n\n   <chr>                  <int>\n 1 Venezuela                253\n 2 Peru                     244\n 3 Dominican Republic       226\n 4 Ecuador                  219\n 5 Madagascar               177\n 6 Blend                    156\n 7 Nicaragua                100\n 8 Bolivia                   80\n 9 Colombia                  79\n10 Tanzania                  79\n# ... with 52 more rows\n\n\n\ndf %>% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ncocoa_percent\n0\n1.00\n3\n6\n0\n46\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nref\n0\n1\n1429.80\n757.65\n5\n802\n1454.00\n2079.0\n2712\n▆▇▇▇▇\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006\n2012\n2015.00\n2018.0\n2021\n▃▅▇▆▅\n\n\nrating\n0\n1\n3.20\n0.45\n1\n3\n3.25\n3.5\n4\n▁▁▅▇▇\n\n\n\n\n\n\ndf %>%\n  ggplot(aes(rating)) +\n  geom_histogram(binwidth = .25, fill = \"midnightblue\") +\n  labs(x = \"Chocolate bar rating\",\n       y = \"Number of bars\")\n\n\n\n\nInitial questions\n\nHave ratings been going up over time?\n\nThe first simple plot we can make to examine this question is a boxplot -\n\ndf %>% \n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_boxplot()\n\n\n\n\n\nlibrary(ggridges)\n\ndf %>%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\nPicking joint bandwidth of 0.14\n\n\n\n\n\nWhat can we learn about the number of ingredients and cocoa percentage?\n\ndf %>% \n  select(ingredients)\n\n# A tibble: 2,530 x 1\n   ingredients\n   <chr>      \n 1 3- B,S,C   \n 2 3- B,S,C   \n 3 3- B,S,C   \n 4 3- B,S,C   \n 5 3- B,S,C   \n 6 3- B,S,C   \n 7 3- B,S,C   \n 8 4- B,S,C,L \n 9 4- B,S,C,L \n10 4- B,S,C,L \n# ... with 2,520 more rows\n\ndf %>% \n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>% \n  # select(n_ingredients, ingredients) %>% \n  mutate(n_ingredients = parse_number(n_ingredients))\n\n# A tibble: 2,530 x 11\n     ref company_manufacturer company_location review_date country_of_bean_orig~\n   <dbl> <chr>                <chr>                  <dbl> <chr>                \n 1  2454 5150                 U.S.A.                  2019 Tanzania             \n 2  2458 5150                 U.S.A.                  2019 Dominican Republic   \n 3  2454 5150                 U.S.A.                  2019 Madagascar           \n 4  2542 5150                 U.S.A.                  2021 Fiji                 \n 5  2546 5150                 U.S.A.                  2021 Venezuela            \n 6  2546 5150                 U.S.A.                  2021 Uganda               \n 7  2542 5150                 U.S.A.                  2021 India                \n 8   797 A. Morin             France                  2012 Bolivia              \n 9   797 A. Morin             France                  2012 Peru                 \n10  1011 A. Morin             France                  2013 Panama               \n# ... with 2,520 more rows, and 6 more variables:\n#   specific_bean_origin_or_bar_name <chr>, cocoa_percent <chr>,\n#   n_ingredients <dbl>, ingredients <chr>,\n#   most_memorable_characteristics <chr>, rating <dbl>\n\n# jpeg(filename = \"figures/Chocolate_bar_ratings.jpeg\",\n#      height = 6,\n#      width = 8,\n#      units = \"in\",\n#      res = 1000)\n\ndf %>% \n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>% \n  mutate(across(c(n_ingredients, cocoa_percent), parse_number),\n         cocoa_percent = cocoa_percent - cocoa_percent %% 5) %>% \n  count(cocoa_percent, n_ingredients) %>% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars\")\n\nWarning: Removed 10 rows containing missing values (geom_tile).\n\n\n\n\n# dev.off()\n\ndf %>% \n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>% \n  mutate(cocoa_percent = parse_number(cocoa_percent),\n         cocoa_percent = round(cocoa_percent, 1)) %>% \n  count(cocoa_percent, n_ingredients) %>% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\nWhat do we know about the countries of origin?\n\ndf %>% \n  add_count(country_of_bean_origin) %>%\n  # only include countries with more than 60 bars\n  filter(n > 60) %>% \n  group_by(country_of_bean_origin) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %>% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\nWhat are some fun variables??\n\ndf %>% \n  select(most_memorable_characteristics, rating) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(across(most_memorable_characteristics, str_squish)) %>% \n  add_count(most_memorable_characteristics) %>% \n  filter(n > 15) %>% \n  group_by(most_memorable_characteristics) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  ungroup() %>% \n  arrange(mean_rating) %>% \n  slice(1:10, 69:78)\n\n# A tibble: 20 x 2\n   most_memorable_characteristics mean_rating\n   <chr>                                <dbl>\n 1 chemical                              2.5 \n 2 medicinal                             2.55\n 3 off notes                             2.58\n 4 cardboard                             2.59\n 5 burnt                                 2.72\n 6 bitter                                2.72\n 7 rubber                                2.73\n 8 pungent                               2.74\n 9 metallic                              2.76\n10 chalky                                2.78\n11 berry                                 3.36\n12 banana                                3.38\n13 cocoa                                 3.38\n14 raisins                               3.38\n15 melon                                 3.40\n16 nuts                                  3.40\n17 honey                                 3.42\n18 dried fruit                           3.44\n19 rich cocoa                            3.44\n20 rich                                  3.46\n\n\nWord model\n\ndf_characteristics <- df %>% \n  select(c(most_memorable_characteristics, rating)) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\ndf_characteristics %>% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 972 x 2\n   most_memorable_characteristics     n\n   <chr>                          <int>\n 1 sweet                            273\n 2 nutty                            261\n 3 cocoa                            252\n 4 roasty                           213\n 5 earthy                           190\n 6 creamy                           189\n 7 sandy                            170\n 8 fatty                            166\n 9 floral                           146\n10 intense                          141\n# ... with 962 more rows\n\n\n\ndf_characteristics %>% \n  group_by(most_memorable_characteristics) %>% \n  add_count() %>% \n  filter(n > 3) %>% \n  mutate(avg_rating = mean(rating)) %>% \n  ungroup() %>% \n  distinct(most_memorable_characteristics, avg_rating) %>% \n  slice_max(avg_rating, n = 12, with_ties = F) %>% \n    mutate(avg_rating = round(avg_rating, 2)) %>% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\ndark berry\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.1.4 --\n\n\nv broom        0.7.11     v rsample      0.1.1 \nv dials        0.0.10     v tune         0.1.6 \nv infer        1.0.0      v workflows    0.2.4 \nv modeldata    0.1.1      v workflowsets 0.1.0 \nv parsnip      0.1.7      v yardstick    0.0.8 \nv recipes      0.1.17     \n\n\nWarning: package 'broom' was built under R version 4.1.2\n\n\nWarning: package 'rsample' was built under R version 4.1.2\n\n\nWarning: package 'workflows' was built under R version 4.1.2\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n* Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(textrecipes)\n\nWarning: package 'textrecipes' was built under R version 4.1.2\n\ndf_characteristics_folds <- vfold_cv(df_characteristics)\n\nglmnet_recipe <- \n  recipe(formula = rating ~ ., data = df_characteristics) %>% \n  step_tokenize(most_memorable_characteristics) %>% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% \n  step_tf(most_memorable_characteristics) %>% \n  step_normalize(all_predictors(), -all_nominal())\n\nglmnet_recipe %>% prep() %>% juice()\n\n# A tibble: 7,065 x 101\n   rating tf_most_memorable_~ tf_most_memorab~ tf_most_memorab~ tf_most_memorab~\n    <dbl>               <dbl>            <dbl>            <dbl>            <dbl>\n 1   3.25             -0.0764          -0.0653          -0.0818          -0.0519\n 2   3.25             -0.0764          -0.0653          -0.0818          -0.0519\n 3   3.25             -0.0764          -0.0653          -0.0818          -0.0519\n 4   3.5              -0.0764          -0.0653          -0.0818          -0.0519\n 5   3.5              -0.0764          -0.0653          -0.0818          -0.0519\n 6   3.5              -0.0764          -0.0653          -0.0818          -0.0519\n 7   3.75             -0.0764          -0.0653          -0.0818          -0.0519\n 8   3.75             -0.0764          -0.0653          -0.0818          -0.0519\n 9   3.75             -0.0764          -0.0653          -0.0818          -0.0519\n10   3                -0.0764          -0.0653          -0.0818          -0.0519\n# ... with 7,055 more rows, and 96 more variables:\n#   tf_most_memorable_characteristics_banana <dbl>,\n#   tf_most_memorable_characteristics_base <dbl>,\n#   tf_most_memorable_characteristics_basic <dbl>,\n#   tf_most_memorable_characteristics_berry <dbl>,\n#   tf_most_memorable_characteristics_bitter <dbl>,\n#   tf_most_memorable_characteristics_black <dbl>, ...\n\n\n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\nWarning: package 'glmnet' was built under R version 4.1.2\n\n\n! Fold01: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold02: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold03: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold04: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold05: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold06: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold07: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold08: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold09: internal: A correlation computation is required, but `estimate` is const...\n\n\n! Fold10: internal: A correlation computation is required, but `estimate` is const...\n\nglmnet_tune %>% \n  autoplot()\n\n\n\n\n\nglmnet_model_final <- finalize_workflow(glmnet_workflow, glmnet_tune %>% \n  select_best())\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nfinal_fit <- glmnet_model_final %>% \n  fit(df_characteristics)\n\n\nfinal_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %>%\n  mutate(sign = estimate > 0) %>%\n  group_by(sign) %>%\n  mutate(estimate = abs(estimate)) %>% \n  slice_max(estimate, n = 12) %>%\n  ungroup() %>%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %>% \n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")"
  },
  {
    "objectID": "code/Indoor_air_pollution.html",
    "href": "code/Indoor_air_pollution.html",
    "title": "indoor air pollution",
    "section": "",
    "text": "df %>% \n  filter(entity == \"Sweden\") %>% \n  ggplot(aes(year, death_rate)) +\n  geom_line()\n\n\n\ncountries <- df %>% distinct(entity) %>% slice_sample(n = 6)\n\ndf %>% \n  filter(entity %in% countries$entity) %>% \n  ggplot(aes(year, death_rate, colour = entity)) +\n  geom_line() +\n  geom_point() +\n  gghighlight::gghighlight()\n\nlabel_key: entity\n\n\n\n\n\nWhich countries have seen the greatest decline?\n\ndf %>% \n  group_by(entity) %>% \n  filter(year %in% c(2019, 1990)) %>% \n  ungroup() %>% \n  pivot_wider(names_from = year, values_from = death_rate) %>% \n  mutate(delta = `2019` - `1990`,\n         pct_change = delta / `1990`) %>% \n  arrange(pct_change) %>% \n  head(20) %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nentity\ncode\n1990\n2019\ndelta\npct_change\n\n\n\nSaudi Arabia\nSAU\n76.8869317\n0.2305187\n-76.6564130\n-0.9970018\n\n\nUnited Arab Emirates\nARE\n1.0183772\n0.0043776\n-1.0139996\n-0.9957014\n\n\nEgypt\nEGY\n27.6665858\n0.1271768\n-27.5394090\n-0.9954032\n\n\nOman\nOMN\n104.2532668\n0.5065138\n-103.7467530\n-0.9951415\n\n\nLibya\nLBY\n27.9377343\n0.1628280\n-27.7749063\n-0.9941718\n\n\nIraq\nIRQ\n49.6047848\n0.3052765\n-49.2995083\n-0.9938458\n\n\nIran\nIRN\n20.2390322\n0.1445276\n-20.0945046\n-0.9928590\n\n\nSouth Korea\nKOR\n2.2729526\n0.0181038\n-2.2548489\n-0.9920351\n\n\nJordan\nJOR\n3.3276076\n0.0271345\n-3.3004731\n-0.9918457\n\n\nAlgeria\nDZA\n28.3232504\n0.2333894\n-28.0898610\n-0.9917598\n\n\nSingapore\nSGP\n3.5794998\n0.0300034\n-3.5494964\n-0.9916180\n\n\nSyria\nSYR\n19.6628117\n0.1677271\n-19.4950846\n-0.9914698\n\n\nTunisia\nTUN\n21.0457556\n0.1840775\n-20.8616781\n-0.9912535\n\n\nLebanon\nLBN\n19.0212379\n0.1857972\n-18.8354407\n-0.9902321\n\n\nQatar\nQAT\n0.7429927\n0.0085036\n-0.7344891\n-0.9885549\n\n\nTurkey\nTUR\n24.5144939\n0.2864653\n-24.2280286\n-0.9883145\n\n\nBahrain\nBHR\n17.7962430\n0.2889108\n-17.5073323\n-0.9837656\n\n\nKuwait\nKWT\n1.4343688\n0.0243295\n-1.4100392\n-0.9830382\n\n\nPalestine\nPSE\n72.3896828\n1.4275602\n-70.9621226\n-0.9802795\n\n\nBrunei\nBRN\n13.6930890\n0.3288176\n-13.3642713\n-0.9759866\n\n\n\n\n\nInteractive chart\n\n\n\nChloropleth\n\nPrepare country codes\nDraw base map\nConnect\n\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.1.2\n\n\nWarning: multiple methods tables found for 'direction'\n\n\nWarning: multiple methods tables found for 'gridDistance'\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.1.2\n\n\nLinking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE\n\ndata(\"World\")\n\nworld <- World %>% as_tibble()\n\ndf_map <- df %>% \n    filter(year == 2019) %>% \n    distinct() %>% \n    left_join(world, by = c(\"code\" = \"iso_a3\"))\n\ndf_map %>%\n  st_as_sf() %>%\n  ggplot() +\n  geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n  geom_sf(aes(fill = death_rate)) +\n  scale_fill_viridis_c(trans = \"sqrt\") +\n  labs(fill = \"Death rate from indoor air pollution\") +\n  guides(fill = guide_colorbar(\n    barwidth = 25, barheight = 1,\n    title.position = \"top\",\n    title.hjust = .5\n  )) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nInteractive map that allows you to click through"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Screencasts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nAn exploration of chocolte bar ratings\n\n\n\nJonathan Jayes\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ggplot to recreate a classic climate change visualization.\n\n\n\nJonathan Jayes\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code/Rare_earth_elements.html",
    "href": "code/Rare_earth_elements.html",
    "title": "Untitled",
    "section": "",
    "text": "df <- readxl::read_excel(here::here(\"data\", \"Global_REE_occurrence_database.xlsx\"))\n\ndf <- df %>% \n  janitor::clean_names()\n\nWhere are the lithium deposits??\n\ndf %>% \n  separate_rows(commods, sep = \",\") %>% \n  mutate(commods = str_squish(commods)) %>% \n  filter(commods == \"Li\") %>% \n  count(country, sort = T)\n\n# A tibble: 22 x 2\n   country                n\n   <chr>              <int>\n 1 Uzbekistan             9\n 2 Russian Federation     7\n 3 Canada                 4\n 4 Kazakhstan             4\n 5 China                  3\n 6 Czech Republic         3\n 7 Mongolia               3\n 8 Mozambique             3\n 9 Norway                 3\n10 Tajikistan             3\n# ... with 12 more rows"
  },
  {
    "objectID": "code/Show_your_stripes.html",
    "href": "code/Show_your_stripes.html",
    "title": "Recreating Ed Hawkins’ Show Your Stripes graphic",
    "section": "",
    "text": "Recreate the show your stripes chart - sounds kinda fun and a nice place to showcase some tricks\nThe chart is shown at:\n\nknitr::include_url(\"https://showyourstripes.info/\")\n\n\n\n\nThe Met Office has a number of different series on their website here: https://www.metoffice.gov.uk/hadobs/hadcrut5/data/current/download.html\n“Time series are presented as temperature anomalies (deg C) relative to 1961-1990.”\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nMonthly data?\n\ndf <- read_csv(\"https://www.metoffice.gov.uk/hadobs/hadcrut5/data/current/analysis/diagnostics/HadCRUT.5.0.1.0.analysis.summary_series.global.monthly.csv\")\n\nRows: 2066 Columns: 4\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Time\ndbl (3): Anomaly (deg C), Lower confidence limit (2.5%), Upper confidence li...\n\n\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf <- df %>% \n  janitor::clean_names()\n\n\nLet’s begin by looking at the dataset with the skimr package:\n\ndf %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2066\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ntime\n0\n1\n7\n7\n0\n2066\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nanomaly_deg_c\n0\n1\n-0.08\n0.38\n-1.04\n-0.35\n-0.16\n0.11\n1.22\n▁▇▅▂▁\n\n\nlower_confidence_limit_2_5_percent\n0\n1\n-0.22\n0.44\n-1.22\n-0.55\n-0.31\n0.00\n1.18\n▁▇▅▂▁\n\n\nupper_confidence_limit_97_5_percent\n0\n1\n0.06\n0.34\n-0.87\n-0.17\n0.00\n0.22\n1.26\n▁▇▆▂▁\n\n\n\n\n\n\ndf %>% \n  ggplot(aes(time, anomaly_deg_c)) +\n  geom_point()\n\n\n\n\nThe x-axis is a bit of a mess!\nLet’s rather try to change these to a date with the help of the lubridate package.\n\ndf <- df %>% \n  mutate(time = lubridate::ymd(paste0(time, \"-01\")))\n\ndf %>%\n  ggplot(aes(time, anomaly_deg_c)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0, lty = 2)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWow! That’s way better - and now we have\n\n\ndf %>% \n  filter(str_detect(time, \"\\\\d\\\\d\\\\d\\\\d-01.*\")) %>% \n  ggplot(aes(x = time, ymin = lower_confidence_limit_2_5_percent, ymax = upper_confidence_limit_97_5_percent)) +\n  geom_ribbon(alpha = .5) +\n  geom_line(aes(x = time, y = anomaly_deg_c))\n\n\n\n\nWhat about averaging across the year? We can make use of the lubridate package again! This time we use the year function to get the year (as a number) out of the time column.\n\ndf %>% \n  mutate(year = lubridate::year(time)) %>% \n  group_by(year) %>% \n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>% \n  ungroup() %>% \n  distinct(year, .keep_all = T) %>% \n  ggplot(aes(x = time, ymin = lower_confidence_limit_2_5_percent, ymax = upper_confidence_limit_97_5_percent)) +\n  geom_ribbon(fill = \"grey70\") +\n  geom_line(aes(x = time, y = anomaly_deg_c))\n\n\n\n\nWe can see that as time goes on, we get better at measuring things with a lower margin of error.\n\n\ndf %>% \n  ggplot(aes(time, y = 1, fill = anomaly_deg_c)) + \n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  labs(x = \"Year\",\n       y = NULL,\n       caption = \"Inspired by Ed Hawkins\\nData from the Met Office\")\n\n\n\n\n\ndf %>%\n  mutate(year = lubridate::year(time)) %>%\n  group_by(year) %>%\n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>%\n  ungroup() %>%\n  distinct(year, .keep_all = T) %>%\n  ggplot(aes(time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspired by Ed Hawkins\\nData from the Met Office\"\n  )\n\n\n\n\n\n\ndf %>%\n  ggplot(aes(time, anomaly_deg_c, fill = anomaly_deg_c)) +\n  geom_col() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  geom_hline(yintercept = 0, lty = 2) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(barwidth = 25, barheight = 1, title.position = \"top\", title.hjust = 0.5))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kathy’s coding club",
    "section": "",
    "text": "This is a website that collects the contents of Kathy’s coding club"
  }
]