[
  {
    "objectID": "code/Gender_equality_index_just_interactive_chart.html",
    "href": "code/Gender_equality_index_just_interactive_chart.html",
    "title": "Gender Equality Just Interactive Chart",
    "section": "",
    "text": "library(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(here)\n\nhere() starts at C:/Users/User/Documents/Recon/screencasts\n\ntheme_set(theme_light())"
  },
  {
    "objectID": "code/Gender_equality_index_just_interactive_chart.html#purpose",
    "href": "code/Gender_equality_index_just_interactive_chart.html#purpose",
    "title": "Gender Equality Just Interactive Chart",
    "section": "Purpose",
    "text": "Purpose\nJust recreate the interactive chart from the Economist:\n\nRead in data\nSelect just what we want for.\n\ndf <- read_rds(here(\"data/Gender-equality-index.rds\")) \n\ndf <- df %>% select(reference_year_main, country_name, gender_equality_index) %>% \n  distinct() %>% \n  rename(year = reference_year_main)\n\n\n\nEDA\n\ndf %>% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n168\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncountry_name\n0\n1\n5\n14\n0\n28\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2015.17\n3.25\n2010.00\n2012.00\n2016.00\n2018.00\n2019.00\n▃▃▃▃▇\n\n\ngender_equality_index\n0\n1\n62.87\n8.68\n48.61\n55.48\n61.84\n69.32\n83.94\n▇▇▆▆▂\n\n\n\n\n\n\ndf %>%\n  ggplot(aes(year, gender_equality_index, colour = country_name)) +\n  geom_point(cex = 3) +\n  geom_line(cex = 1) +\n  gghighlight::gghighlight(country_name %in% c(\"France\", \"Sweden\", \"Hungary\")) +\n  scale_x_continuous(labels = scales::number_format(big.mark = \"\", accuracy = 1)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    x = \"Year\",\n    y = \"Gender Equality Index\",\n    caption = \"Data: European Institute of Gender Equality\",\n    title = \"Gender Equality Index\",\n    subtitle = \"European Union member states 2010-2019\"\n  )\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\nTried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: country_name\n\n\n\n\n\nWe want to compare changes in country positions over time, rather than absolute values for the gender equality index.\nWe can use the rank function from dplyr to calculate the rank of each country across our 6 years from 2010 to 2019. The desc function within the rank function means that our highest scoring nation, Sweden, will get the value 1, rather than 28 in our rank column.\n\ndf <- df %>% \n  group_by(year) %>% \n  mutate(rank = rank(desc(gender_equality_index), ties.method = \"average\")) %>% \n  ungroup()\n\ndf %>% \n  arrange(year, rank)\n\n# A tibble: 168 x 4\n    year country_name   gender_equality_index  rank\n   <dbl> <chr>                          <dbl> <dbl>\n 1  2010 Sweden                          80.1     1\n 2  2010 Denmark                         75.2     2\n 3  2010 Netherlands                     74.0     3\n 4  2010 Finland                         73.1     4\n 5  2010 Belgium                         69.3     5\n 6  2010 France                          67.5     6\n 7  2010 Spain                           66.4     7\n 8  2010 Ireland                         65.4     8\n 9  2010 European Union                  63.1     9\n10  2010 Slovenia                        62.7    10\n# ... with 158 more rows\n\n\nWe can draw our first version of the plot we want to create! We will again highlight our three comparator countries to reduce the clutter on the chart.\n\n\n\ndf %>% \n  ggplot(aes(year, rank, color = country_name)) +\n  geom_line(size = 3) +\n  gghighlight::gghighlight(country_name %in% c(\"France\", \"Sweden\", \"Hungary\"))\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: country_name\n\n\n\n\n\n\nNotice here that Sweden, our highest ranked country, is at the bottom of the y-axis. Instead, we should invert the y-axis with the ggplot call scale_y_reverse().\n\n\n\n\ndf %>% \n  ggplot(aes(year, rank, color = country_name)) +\n  geom_line(size = 3) +\n  gghighlight::gghighlight(country_name %in% c(\"France\", \"Sweden\", \"Hungary\")) +\n  scale_y_reverse()\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: country_name\n\n\n\n\n\n\n\n\nlabels_left <- df %>% \n  filter(year == min(year)) %>% \n  mutate(left_rank = rank) %>% \n  select(country_name, left_rank)\n\nlabels_right <- df %>% \n  filter(year == max(year)) %>% \n  mutate(right_rank = rank) %>% \n  select(country_name, right_rank)\n\ndf <- df %>% \n  inner_join(labels_left) %>% \n  inner_join(labels_right)\n\nJoining, by = \"country_name\"\nJoining, by = \"country_name\""
  },
  {
    "objectID": "code/Show_your_stripes.html",
    "href": "code/Show_your_stripes.html",
    "title": "Recreating Ed Hawkins’ Show Your Stripes graphic",
    "section": "",
    "text": "Clean environment in Rstuido Folder path in Rstudio"
  },
  {
    "objectID": "code/Show_your_stripes.html#welcome",
    "href": "code/Show_your_stripes.html#welcome",
    "title": "Recreating Ed Hawkins’ Show Your Stripes graphic",
    "section": "Welcome",
    "text": "Welcome\nHello and welcome to Kathy’s coding club. Today we will be recreating a classic climate change visualization, the show your stripes graphic by Ed Hawkins. This graphic shows how the global temperature has increased over time, going from blue to red on the colour scale.\nIt is a powerful reminder of how our human activity has resulted in climate change.\nIn this video we’re going to ingest the data into R, do some exploratory analysis, and the recreate the visualization in ggplot.\nAs usual, if you have any questions about the commands that we are using, please feel free to leave a comment below, or have a look at the accompanying post linked in the description that includes the code used in the analysis which follows.\nAs you can see, in this post you can click on any of the commands and be taken to their documentation to see which package they are from, and how each command works.\n\n\n\n\nRstudio\nLet’s open up Rstudio now and create a fresh Rmarkdown document.\nWe can clear out the boilerplate code that is in place to show us how Rmarkdown documents work, and create some scaffolding for our analysis.\nWe’re going to want to include a section on:\n\nreading in the data\nsome exploratory data analysis\nrecreating the figure\nadding interactivity\nData\nThe original visualization is displayed on a website called ShowYourStripes.info, where you can select different regions to see how they have warmed over time. We’re doing to be looking at the entire globe.\nIf you click on the FAQ section, we can see that the global data is sourced from the UK’s met office. We can head on over to their website to understand a bit more about the data and download it.\nWe read that the “Time series are presented as temperature anomalies (deg C) relative to 1961-1990.” This means we aren’t getting an average temperature, but rather the deviation from the average temperature globally in the base period from 1961 to 1990.\nWe can select the global series at monthly level, and copy the link address so as to read it directly into Rstudio.\nThe first thing we will do is load the tidyverse meta package.\nWe can then use the readr package to read in the comma separated value file as a dataframe named df.\nIf we print the dataframe in the console, we can see that it has four columns and 2,066 rows.\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\ntheme_set(theme_light())\n\ndf <- readr::read_csv(\"https://www.metoffice.gov.uk/hadobs/hadcrut5/data/current/analysis/diagnostics/HadCRUT.5.0.1.0.analysis.summary_series.global.monthly.csv\")\n\nRows: 2066 Columns: 4\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (1): Time\ndbl (3): Anomaly (deg C), Lower confidence limit (2.5%), Upper confidence li...\n\n\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIf we ask for the column names we can see that they’re a little bit difficult to work with, containing parentheses and percentage signs.\n\ndf %>% colnames()\n\n[1] \"Time\"                           \"Anomaly (deg C)\"               \n[3] \"Lower confidence limit (2.5%)\"  \"Upper confidence limit (97.5%)\"\n\n\nWe can use the janitor package to clean up the names into camel case, where all of the letters are in lower case and words are separated by underscores.\n\ndf <- df %>% \n  janitor::clean_names()\n\ndf %>% colnames()\n\n[1] \"time\"                                \"anomaly_deg_c\"                      \n[3] \"lower_confidence_limit_2_5_percent\"  \"upper_confidence_limit_97_5_percent\"\n\n\nHelpfully the janitor command clean names also changes the percentage signs to the word percent.\nLet’s now try to get a better feel for the data.\nEDA\nLet’s begin by looking at the dataset with the skimr package:\n\ndf %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2066\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ntime\n0\n1\n7\n7\n0\n2066\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nanomaly_deg_c\n0\n1\n-0.08\n0.38\n-1.04\n-0.35\n-0.16\n0.11\n1.22\n▁▇▅▂▁\n\n\nlower_confidence_limit_2_5_percent\n0\n1\n-0.22\n0.44\n-1.22\n-0.55\n-0.31\n0.00\n1.18\n▁▇▅▂▁\n\n\nupper_confidence_limit_97_5_percent\n0\n1\n0.06\n0.34\n-0.87\n-0.17\n0.00\n0.22\n1.26\n▁▇▆▂▁\n\n\n\n\n\nWe can see that we have three numeric columns and one character column, time. The data is all complete, with no missing values.\nLet’s see if we can plot the data using ggplot.\nWe use ggplot’s aesthetic mapping here, putting time on the x-axis and the temperature anomaly in degrees Celsius on the y-axis.\n\ndf %>% \n  ggplot(aes(x = time, y = anomaly_deg_c)) +\n  geom_point()\n\n\n\n\nThe x-axis is a bit of a mess! That’s because th time variable is stored as a character, rather than a date:\n\ndf %>% select(time)\n\n# A tibble: 2,066 x 1\n   time   \n   <chr>  \n 1 1850-01\n 2 1850-02\n 3 1850-03\n 4 1850-04\n 5 1850-05\n 6 1850-06\n 7 1850-07\n 8 1850-08\n 9 1850-09\n10 1850-10\n# ... with 2,056 more rows\n\n\nLet’s rather try to change these characters to a date with the help of the lubridate package.\nThe lubridate command ymd here will parse a date in the form, year, then month, and then day. Because our data comes with just a year and month value, we can paste a string indicating the first of the month with the paste0 command from base R.\n\ndf <- df %>% \n  mutate(time = lubridate::ymd(paste0(time, \"-01\")))\n\nNow let us try and plot the data again - using the geom_smooth command to add a smoothed line in top of the points.\n\ndf %>%\n  ggplot(aes(x = time, y = anomaly_deg_c)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0, lty = 2)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nWow! That’s way better. We also added a dotted line at y = 0 to show use which values are below and above the baseline temperature level.\nRecreating the figure\nIf we want to recreate the figure from Ed Hawkins, we can use the geom_tile geometry in ggplot.\nIn this case, we are going to map time onto the x-axis, and the deviation from the base temperature to the fill aesthetic. For the y-axis we will just choose an arbitrary constant, in this case 1.\n\ndf %>% \n  ggplot(aes(x = time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile()\n\n\n\n\nNow we’re getting somewhere, we can see that the colour is lighter to the right of the figure - but we need to improve our colour scale in order to match that of the original graphic.\nLet’s add a gradient colour scale with the command scale_fill_gradient2 from ggplot. Here we can specify we want the low values to be blue, the high values red, the mid values white, and the point at which the white is shown as zero.\n\ndf %>%\n  ggplot(aes(x = time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0)\n\n\n\n\nThat’s great! Let’s now remove some of the superfluous elements and add in some labels.\nIn this case, we are going to remove the legend as this graphic is more about the message than the actual values.\n\ndf %>%\n  ggplot(aes(x = time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspired by Ed Hawkins\\nData from the Met Office\"\n  )\n\n\n\n\nFinally, we can remove a bit of the noise in the figure calculating a yearly average from our monthly values. Here we can use the group_by and across function from dplyr.\nWe create a grouping variable with the lubridate::year command, group by year, and then use the across function to calculate the mean for each year for our three numeric variables.\n\ndf <- df %>%\n  mutate(year = lubridate::year(time)) %>%\n  group_by(year) %>%\n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>%\n  ungroup() %>% \n  distinct(year, .keep_all = T)\n\nNext we can reuse the graphics code from above to create a chart that more closely matches the version we saw on showyourstripes.info.\n\ndf %>%\n  ggplot(aes(time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspired by Ed Hawkins\\nData from the Met Office\"\n  )\n\n\n\n\nInteractive chart\nIn addition, if we are going to publish our figure online, it might be fun to introduce a bit of interactivity. This is easily accomplished with the ggiraph package.\nThis package takes static ggplot geometries and makes them interactive such that hovering over the figure shows a data value beside the cursor.\nThere are three steps:\n\nWe modify our graphics code from geom_tile to geom_tile_interactive\nWe add in an argument for the tooltip value in the ggplot aesthetic mapping\nWe assign our graphics code to an R object and call it from inside the command ggiraph.\n\n\nlibrary(ggiraph)\n\nWarning: package 'ggiraph' was built under R version 4.1.2\n\ng <- df %>%\n  ggplot(aes(time, y = 1, fill = anomaly_deg_c, tooltip = anomaly_deg_c)) +\n  geom_tile_interactive() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspired by Ed Hawkins\\nData from the Met Office\"\n  )\n\n\nggiraph(ggobj = g)\n\n\n\n\n\nWe can improve this interactive figure by rounding off the temperature anomaly value, and adding some context to our tooltip:\n\ng <- df %>% \n  mutate(tooltip = round(anomaly_deg_c, digits = 2),\n         tooltip = str_c(\"Degree deviation from base period in \", year, \"\\n\", tooltip, \" c\")) %>% \n  ggplot(aes(time, y = 1, fill = anomaly_deg_c, tooltip = tooltip)) +\n  geom_tile_interactive() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspired by Ed Hawkins\\nData from the Met Office\"\n  )\n\n\nggiraph(ggobj = g)\n\n\n\n\n\nPost script\nWhat about the confidence level?\nWhat about averaging across the year? We can make use of the lubridate package again! This time we use the year function to get the year (as a number) out of the time column.\n\ndf %>%\n  mutate(year = lubridate::year(time)) %>%\n  group_by(year) %>%\n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>%\n  ungroup() %>%\n  distinct(year, .keep_all = T) %>%\n  ggplot(aes(x = time, ymin = lower_confidence_limit_2_5_percent, ymax = upper_confidence_limit_97_5_percent)) +\n  geom_ribbon(fill = \"grey70\") +\n  geom_line(aes(x = time, y = anomaly_deg_c))\n\n\n\n\nWe can see that as time goes on, we get better at measuring things with a lower margin of error.\nBars with scale\n\ndf %>%\n  ggplot(aes(time, anomaly_deg_c, fill = anomaly_deg_c)) +\n  geom_col() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  geom_hline(yintercept = 0, lty = 2) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(barwidth = 25, barheight = 1, title.position = \"top\", title.hjust = 0.5))"
  },
  {
    "objectID": "exams/econ31_4170_2020h.html",
    "href": "exams/econ31_4170_2020h.html",
    "title": "Data science for economists exams",
    "section": "",
    "text": "The university of Oslo has produced a course called Data Science for Economists at the bachelor level. I want to complete the exams as a resource for Kathy’s coding club, and because it looks fun!\n\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.1.2\n\ntheme_set(theme_light())\n\ninclude_graphics(here::here(\"exams\", \"econ31_4170_2020h.pdf\"))\n\n\n\n\n\n\n\nmobility <- read.csv(here::here(\"exams\", \"4170_vedlegg_2020_no_region_mobility_report.csv\")) %>% \n  as_tibble()\n\nmobility %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n34217\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlogical\n2\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncountry_region_code\n0\n1\n2\n2\n0\n1\n0\n\n\ncountry_region\n0\n1\n6\n6\n0\n1\n0\n\n\nsub_region_1\n0\n1\n0\n23\n277\n12\n0\n\n\nsub_region_2\n0\n1\n0\n28\n3324\n172\n0\n\n\niso_3166_2_code\n0\n1\n0\n5\n31170\n12\n0\n\n\ndate\n0\n1\n10\n10\n0\n277\n0\n\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\nmetro_area\n34217\n0\nNaN\n:\n\n\ncensus_fips_code\n34217\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nretail_and_recreation_percent_change_from_baseline\n21114\n0.38\n-5.79\n23.94\n-96\n-16\n-3\n7.0\n108\n▁▃▇▁▁\n\n\ngrocery_and_pharmacy_percent_change_from_baseline\n21786\n0.36\n2.24\n20.78\n-95\n-5\n4\n12.0\n104\n▁▁▇▁▁\n\n\nparks_percent_change_from_baseline\n29782\n0.13\n54.38\n75.57\n-80\n3\n37\n89.5\n616\n▇▃▁▁▁\n\n\ntransit_stations_percent_change_from_baseline\n18798\n0.45\n-24.98\n27.81\n-92\n-45\n-28\n-8.0\n135\n▃▇▂▁▁\n\n\nworkplaces_percent_change_from_baseline\n1632\n0.95\n-27.89\n19.68\n-92\n-42\n-24\n-13.0\n30\n▁▅▇▇▁\n\n\nresidential_percent_change_from_baseline\n22596\n0.34\n5.36\n5.72\n-11\n2\n4\n8.0\n33\n▁▇▃▁▁\n\n\n\n\nmobility %>% count() -> n_obs\n\nThere are 34217 observations.\n\n# transform into lubridate year month day format.\nmobility <- mobility %>% \n  mutate(date = lubridate::ymd(date))\n\nskimmed <- mobility %>% \n  skimr::skim()\n\nskimmed$Date.min %>% as_tibble() %>% filter(!is.na(value)) -> date_min\nskimmed$Date.max %>% as_tibble() %>% filter(!is.na(value)) -> date_max\n\nThe dates range from 2020-02-15 and 2020-11-17.\n\n\nmobility %>% count(sub_region_1, sort = T)\n\n# A tibble: 12 x 2\n   sub_region_1                  n\n   <chr>                     <int>\n 1 \"Viken\"                    7482\n 2 \"Vestland\"                 3776\n 3 \"Innlandet\"                3520\n 4 \"TrÃ¸ndelag\"               3277\n 5 \"Rogaland\"                 3227\n 6 \"Vestfold og Telemark\"     3061\n 7 \"MÃ¸re og Romsdal\"         2987\n 8 \"Nordland\"                 2250\n 9 \"Agder\"                    2242\n10 \"Troms og Finnmark fylke\"  1841\n11 \"\"                          277\n12 \"Oslo\"                      277\n\nmobility %>% count(sub_region_2, sort = T)\n\n# A tibble: 172 x 2\n   sub_region_2                    n\n   <chr>                       <int>\n 1 \"\"                           3324\n 2 \"Bergen Municipality\"         277\n 3 \"Trondheim Municipality\"      277\n 4 \"Ullensaker Municipality\"     277\n 5 \"Stavanger Municipality\"      275\n 6 \"Drammen Municipality\"        274\n 7 \"Kristiansand Municipality\"   274\n 8 \"LillestrÃ¸m\"                 273\n 9 \"TromsÃ¸ Municipality\"        273\n10 \"Ã…lesund Municipality\"       271\n# ... with 162 more rows\n\n\nUTF-8 can accommodate Norwegian characters like Ã.\n\n\noslo <- mobility %>% \n  filter(sub_region_1 == \"Oslo\")\n\noslo %>% \n  ggplot(aes(date, residential_percent_change_from_baseline)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nIn Oslo, it looks like there was a big spike in March and a decline in movement over the summer, before an increase again towards Christmas.\nThere appear to be some day of the week effects also!\n\n\noslo <- oslo %>%\n  mutate(\n    weekday = lubridate::wday(date, label = T),\n    weekend = case_when(\n      weekday %in% c(\"Sat\", \"Sun\") ~ 1,\n      TRUE ~ 0\n    ),\n    weekend = factor(weekend)\n  )\n\n\nplot_oslo <- function(var) {\n  oslo %>%\n    ggplot(aes(date, residential_percent_change_from_baseline, colour = {{ var }})) +\n    geom_point() +\n    geom_smooth(se = F)\n}\n\nplot_oslo(weekday) +\n  labs(colour = \"Weekday\")\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\nplot_oslo(weekend) +\n  labs(colour = \"Is it a weekend?\")\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n\nNote that they store aggregate data in a strange way - if sub_region_2 is an empty string but sub_region_1 names a county then this is aggregate data at the county level.\n\nmobility %>%\n  filter(sub_region_2 == \"\",\n         sub_region_1 != \"\") %>% \n  mutate(\n    weekday = lubridate::wday(date, label = T),\n    weekend = case_when(\n      weekday %in% c(\"Sat\", \"Sun\") ~ \"Weekend\",\n      TRUE ~ \"Weekday\"\n    )\n  ) %>% \n  ggplot(aes(date, residential_percent_change_from_baseline, colour = sub_region_1)) +\n  geom_line() +\n  # geom_smooth(se = F) +\n  facet_wrap(~ weekend, nrow = 2)\n\n\n\n\nThree weekday spikes are between April and June?\n\nIs this correct? Or should I be looking at squared differences?\n\nmobility %>%\n  filter(\n    sub_region_2 == \"\",\n    sub_region_1 != \"\",\n    between(date, lubridate::ymd(\"2020-03-12\"), lubridate::ymd(\"2020-10-31\"))\n  ) %>% \n  group_by(sub_region_1) %>% \n  summarise(mean_response = mean(residential_percent_change_from_baseline, na.rm = T),\n            median_response = median(residential_percent_change_from_baseline, na.rm = T)) %>% \n  ungroup() %>% \n  mutate(sub_region_1 = fct_reorder(sub_region_1, mean_response)) %>% \n  pivot_longer(-sub_region_1) %>% \n  mutate(value = round(value, 2)) %>% \n  ggplot(aes(value, sub_region_1, fill = name)) +\n  geom_col() +\n  geom_text(aes(label = value), hjust = 1.2) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  facet_wrap(~ name) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Summary of residential percent change from baseline\")\n\n\n\n\n\n\nnational <- mobility %>% \n  filter(sub_region_1 == \"\")\n\noslo_compare <- national %>% \n  select(residential_percent_change_from_baseline, date) %>% \n  rename(national = residential_percent_change_from_baseline) %>% \n  bind_cols(oslo %>% select(residential_percent_change_from_baseline) %>% \n              rename(oslo = residential_percent_change_from_baseline))\n\noslo_compare %>% \n  pivot_longer(-date) %>% \n  ggplot(aes(date, value, colour = name)) +\n  geom_line()\n\n\n\noslo_compare %>% \n  mutate(week = lubridate::week(date)) %>% \n  group_by(week) %>% \n  mutate(across(c(national, oslo), mean)) %>% \n  ungroup() %>% \n  distinct(national, oslo, week) %>% \n  mutate(oslo_national = oslo / national) %>% \n  ggplot(aes(week, oslo_national)) +\n  geom_line()\n\n\n\n\n???\n\n\n\ndf <- tibble(x = 1:10,\n             y = 1:10)\n\ngamma = .5\ntheta = .3\nphi = 2\n\ndf %>% \n  mutate(comp_1 = ((x^(1-gamma)/1-gamma)),\n         comp_2 = phi*((y^(1-theta)/1-theta)),\n         utility = comp_1 + comp_2) %>% \n  pivot_longer(-c(x, y)) %>% \n  ggplot(aes(x, value, colour = name)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\nAxis is not from zero! Sneaky!\n\ndf <- tibble(chrome = 29.619,\n             edge = 31.786,\n             firefox = 26.876)\n\ndf %>% \n  pivot_longer(cols = everything()) %>% \n  mutate(name = fct_reorder(name, value)) %>% \n  ggplot(aes(value, name, fill = name)) +\n  geom_col(show.legend = F) +\n  labs(x = \"Speed score\",\n       y = NULL)\n\n\n\n\n\nSimilar!\n\ndf <- tibble(Latvia = 5.5,\n             Australia = 5.4,\n             Scotland = 5.4,\n             Peru = 5.4,\n             `South Africa` = 5.2,\n             India = 5)\n\ndf %>% \n  pivot_longer(cols = everything()) %>% \n  mutate(name = fct_reorder(name, value)) %>% \n  ggplot(aes(value, name, fill = name)) +\n  geom_col(show.legend = F) +\n  labs(x = \"Average female height\",\n       y = NULL)"
  },
  {
    "objectID": "pipeline/Ideas.html",
    "href": "pipeline/Ideas.html",
    "title": "Ideas",
    "section": "",
    "text": "knitr::include_url(\"https://www.economist.com/graphic-detail/glass-ceiling-index\")\n\n\n\n\n\n\nTo recreate the chart shown in the economist\nResearch article:\n\nknitr::include_url(\"https://www.nature.com/articles/s41558-021-01263-8#data-availability\")\n\n\n\nReplication data:\n\nknitr::include_url(\"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/D2STBL\")"
  },
  {
    "objectID": "pipeline/Intro.html",
    "href": "pipeline/Intro.html",
    "title": "Intro and such",
    "section": "",
    "text": "David Robinson\nHi - I’m Jonathan and welcome to another one of my screencasts.\nAs usual, the data comes from…\nLet’s see what we are looking at this week.\n\n\nJulia Silge\nhi my name is julia silgi and i’m a data scientist and software engineer at R studio and today in this screencast we’re going to use some recent tidy tuesday data on collegiate sports in the u.s and .\n\n\nMicrophone in view\nI think this is fine because the audio quality is better.\nI also think standing is nice because it means that my voice is more clear.\n\n\nAndrew Couch\nhey y’all it’s andrew couch here and in this tidy tuesday video we’re not gonna be looking at this week’s data set instead we’re gonna build upon our tidy models videos."
  },
  {
    "objectID": "rmd/Show_your_stripes.html",
    "href": "rmd/Show_your_stripes.html",
    "title": "Untitled",
    "section": "",
    "text": "EDA\n\ndf %>% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2066\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntime\n0\n1\n7\n7\n0\n2066\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nanomaly_deg_c\n0\n1\n-0.08\n0.38\n-1.04\n-0.35\n-0.16\n0.11\n1.22\n▁▇▅▂▁\n\n\nlower_confidence_limit_2_5_percent\n0\n1\n-0.22\n0.44\n-1.22\n-0.55\n-0.31\n0.00\n1.18\n▁▇▅▂▁\n\n\nupper_confidence_limit_97_5_percent\n0\n1\n0.06\n0.34\n-0.87\n-0.17\n0.00\n0.22\n1.26\n▁▇▆▂▁\n\n\n\n\n\n\ndf %>% \n  ggplot(aes(x = time, y = anomaly_deg_c)) +\n  geom_point()\n\n\n\n\n\ndf %>% \n  select(time)\n\n# A tibble: 2,066 x 1\n   time   \n   <chr>  \n 1 1850-01\n 2 1850-02\n 3 1850-03\n 4 1850-04\n 5 1850-05\n 6 1850-06\n 7 1850-07\n 8 1850-08\n 9 1850-09\n10 1850-10\n# ... with 2,056 more rows\n\n\n\ndf <- df %>% \n  mutate(time = lubridate::ymd(paste0(time, \"-01\")))\n\n\ndf %>% \n  ggplot(aes(x = time, y = anomaly_deg_c)) +\n  geom_point() +\n  geom_smooth() +\n  geom_hline(yintercept = 0, lty = 2)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\nRecreating the figure\n\ndf %>% \n  ggplot(aes(x = time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(x = \"Year\",\n       y = NULL,\n       caption = \"Inspred by Ed Hawkins\\nData from the Met Office\")\n\n\n\n\n\ndf <- df %>% \n  mutate(year = lubridate::year(time)) %>% \n  group_by(year) %>% \n  mutate(across(anomaly_deg_c:upper_confidence_limit_97_5_percent, mean)) %>% \n  ungroup() %>% \n  distinct(year, .keep_all = T)\n\n\ndf %>% \n    ggplot(aes(x = time, y = 1, fill = anomaly_deg_c)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()) +\n  labs(x = \"Year\",\n       y = NULL,\n       caption = \"Inspred by Ed Hawkins\\nData from the Met Office\")\n\n\n\n\n\n\nInteractivity\n\nlibrary(ggiraph)\n\nWarning: package 'ggiraph' was built under R version 4.1.2\n\ng <- df %>%\n  mutate(tooltip = str_c(\"Deviation from base period in \", year, \"\\n\", round(anomaly_deg_c, 2), \" c\")) %>% \n  ggplot(aes(x = time, y = 1, fill = anomaly_deg_c, tooltip = tooltip)) +\n  geom_tile_interactive() +\n  scale_fill_gradient2(\n    low = \"blue\", high = \"red\", mid = \"white\",\n    midpoint = 0\n  ) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  labs(\n    x = \"Year\",\n    y = NULL,\n    caption = \"Inspred by Ed Hawkins\\nData from the Met Office\"\n  )\n\nggiraph(ggobj = g)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Screencasts",
    "section": "",
    "text": "An exploration of chocolte bar ratings\n\n\n\nJonathan\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShowcasing web scraping in R\n\n\n\nJonathan\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ggplot to recreate a classic climate change visualization.\n\n\n\nJonathan\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code/Gender_equality_index_just_interactive_chart.html#building-up-the-chart",
    "href": "code/Gender_equality_index_just_interactive_chart.html#building-up-the-chart",
    "title": "Gender Equality Just Interactive Chart",
    "section": "Building up the chart",
    "text": "Building up the chart\nWhat are the steps?\n\nBase chart\nRemove\nText labels for rankings\nExpand axis legend\nMaybe axis breaks on x-axis?\n\n\ndf %>% \n  ggplot(aes(x = year, y = rank, colour = right_rank, group = country_name)) +\n  geom_line() +\n  theme(legend.position = \"none\")\n\n\n\n\n\ndf %>% \n  ggplot(aes(x = year, y = rank, colour = right_rank, group = country_name)) +\n  geom_line(size =3) +\n  scale_color_gradient2(low = \"blue\", high = \"red\", mid = \"pink\", midpoint = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\ndf %>%\n  ggplot(aes(year, rank, color = right_rank, group = country_name)) +\n  geom_line(size = 2.8, aes(year, rank, group = country_name), colour = \"black\") +\n  geom_line(size = 2) +\n  geom_text(aes(\n    x = 2010, y = left_rank,\n    label = paste0(left_rank, \". \", country_name)\n  ),\n  colour = \"black\",\n  hjust = 1.1,\n  cex = 3\n  ) +\n  geom_text(aes(\n    x = 2019, y = right_rank,\n    label = paste0(right_rank, \". \", country_name)\n  ),\n  colour = \"black\",\n  hjust = 0,\n  cex = 3\n  ) +\n  scale_y_reverse() +\n  scale_color_gradient2(\n    low = \"blue\", high = \"red\",\n    mid = \"pink\",\n    midpoint = 12\n  ) +\n  scale_x_continuous(labels = scales::number_format(big.mark = \"\", accuracy = 1)) +\n  coord_cartesian(xlim = c(2009, 2020)) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  labs(\n    x = NULL,\n    y = NULL\n  )"
  },
  {
    "objectID": "code/Mantalsregister_scraper.html",
    "href": "code/Mantalsregister_scraper.html",
    "title": "Mantalsregister Scraper",
    "section": "",
    "text": "Scrape the website of the Stockholm city archive."
  },
  {
    "objectID": "code/Mantalsregister_scraper.html#structure",
    "href": "code/Mantalsregister_scraper.html#structure",
    "title": "Mantalsregister Scraper",
    "section": "Structure",
    "text": "Structure\n\nLook at the webpage\n\nView page source\n\n\n\nCheck permission\n\nRobots.txt file\n\n\n\nFunction to get the table elements\n\nUsing rvest to get the table elements\n\n\n\nGet the number of pages on which there is data\n\nWrite a neat little function for this\n\n\n\nCreate a workflow to go through\n\nInclude parameters that we can change\n\n\n\nIterate through the list of pages\n\nUsing the map function\n\n\n\nSave the data to a csv file\n\nSome data management"
  },
  {
    "objectID": "code/US_solar_wind.html",
    "href": "code/US_solar_wind.html",
    "title": "US Solar/Wind",
    "section": "",
    "text": "Not sure yet - just needed the comfort of Rstudio\n\nlibrary(tidyverse)\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v purrr   0.3.4\nv tibble  3.1.6     v dplyr   1.0.7\nv tidyr   1.1.4     v stringr 1.4.0\nv readr   2.1.1     v forcats 0.5.1\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'readr' was built under R version 4.1.2\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\n\ndf <- read.csv(here::here(\"data\", \"solar_panels_sweden.csv\")) %>% as_tibble() %>% janitor::clean_names()\n\ndf <- df %>% \n  mutate(across(.cols = c(4, 5), .fns = parse_number)) %>% \n  separate(region, into = c(\"region_code\", \"region\"), sep = \" \")\n\nWarning: 271 parsing failures.\nrow col expected actual\n 61  -- a number     ..\n 62  -- a number     ..\n 81  -- a number     ..\n 82  -- a number     ..\n141  -- a number     ..\n... ... ........ ......\nSee problems(...) for more details.\n\nWarning: 271 parsing failures.\nrow col expected actual\n 61  -- a number     ..\n 62  -- a number     ..\n 81  -- a number     ..\n 82  -- a number     ..\n141  -- a number     ..\n... ... ........ ......\nSee problems(...) for more details.\n\n\nWarning: Expected 2 pieces. Additional pieces discarded in 576 rows [5, 6, 7, 8,\n9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, ...].\n\n\n\ndf <- df %>% \n  mutate(type = case_when(\n    str_count(region_code) == 4 ~ \"kommun\",\n    region == \"Riket\" ~ \"total\",\n    TRUE ~ \"lan\"\n  )) \n\ndf %>% \n  filter(type == \"lan\",\n         effektklass != \"Totalt\") %>% \n  ggplot(aes(solcellsanlaggningar, region, fill = effektklass)) +\n  geom_col()\n\nWarning: Removed 4 rows containing missing values (position_stack).\n\n\n\n\n\n\ndf_kommun <- df %>% \n  filter(type == \"kommun\")\n\ndf_kommun <- df_kommun %>% \n  filter(effektklass == \"Totalt\",\n         ar == 2021)\n\ndf_kommun %>% view\n\n\nmap <- read_rds(here::here(\"data\", \"kommun_map.rds\")) %>% \n  rename(region_code = kn_kod)\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.1.2\n\n\nLinking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE\n\nmap <- map %>%\n  inner_join(df_kommun)\n\nJoining, by = \"region_code\"\n\nmap %>% \n  ggplot() +\n  geom_sf(aes(fill = solcellsanlaggningar)) +\n  scale_fill_viridis_c()"
  },
  {
    "objectID": "code/Chocolate_bar_ratings.html",
    "href": "code/Chocolate_bar_ratings.html",
    "title": "Chocolate bar ratings",
    "section": "",
    "text": "We’re going to be diving into the reviews of chocolate bars from the Manhattan Chocolate Society, who have rated over 2500 chocolate bars from around the world. We are going to be trying to answer a few questions about their ratings.\nThe data is accessible from the R for Data Science Tidy Tuesday repository on Github.\nIf you have any questions about the different commands that we use and why - have a look at this post on my website. You can click on any of the commands and read the documentation to get a better understanding of how they are used, or which package they are from.\nThen let’s get to it - the first step is to open a fresh Rmarkdown document - this allows you to keep human readable notes interspersed with your code that you can return to later, and export the prose and code in a number of different ways to share it, with colleagues or on the internet.\nThe next thing which I think is worth doing is to change the default IDE theme in Rstudio to darkmode. This is easier on your eyes and makes you look professional. You can find the Rstudio appearance settings under tools, and then global options. I like the Cobalt theme, but there are many to choose from.\nOne other thing I would suggest is the use of rainbow parentheses - these make it easy to see where you’re missing a bracket. You can enable them under the code tab in Global options.\nGreat - then let’s get to it!\nFirst we will call the tidyverse meta package - a cohesive group of packages that make working with messy data very easy - originally coined by Hadley Wickham but improved upon by a great many contributors. I’ll leave a link in the description to more info about this.\nThen we read in the data from the R4DS Tidy Tuesday Repo.\n\nlibrary(tidyverse)\n\ndf <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv')\n\ntheme_set(theme_light())\n\nExploratory data analysis\nFirst we can have a look at the data we have read in - This gives us a tibble (similar to a dataframe) with 10 columns (3 numeric and 7 character) and 2,530 individual reviews.\n\ndf %>% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ncocoa_percent\n0\n1.00\n3\n6\n0\n46\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nref\n0\n1\n1429.80\n757.65\n5\n802\n1454.00\n2079.0\n2712\n▆▇▇▇▇\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006\n2012\n2015.00\n2018.0\n2021\n▃▅▇▆▅\n\n\nrating\n0\n1\n3.20\n0.45\n1\n3\n3.25\n3.5\n4\n▁▁▅▇▇\n\n\n\n\n\nNext we can look at some of the common countries in which the beans are processed into bars, as well as where the beans originate.\n\ndf %>% \n  count(company_location, sort = T)\n\n# A tibble: 67 x 2\n   company_location     n\n   <chr>            <int>\n 1 U.S.A.            1136\n 2 Canada             177\n 3 France             176\n 4 U.K.               133\n 5 Italy               78\n 6 Belgium             63\n 7 Ecuador             58\n 8 Australia           53\n 9 Switzerland         44\n10 Germany             42\n# ... with 57 more rows\n\ndf %>% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 x 2\n   country_of_bean_origin     n\n   <chr>                  <int>\n 1 Venezuela                253\n 2 Peru                     244\n 3 Dominican Republic       226\n 4 Ecuador                  219\n 5 Madagascar               177\n 6 Blend                    156\n 7 Nicaragua                100\n 8 Bolivia                   80\n 9 Colombia                  79\n10 Tanzania                  79\n# ... with 52 more rows\n\n\nLooking at the bar ratings, we can see a distribution that is centred just above 3 - this is in line with what we saw from the skim of the dataset above - a mean rating of 3.2\n\ndf %>%\n  ggplot(aes(rating)) +\n  geom_histogram(binwidth = .25, fill = \"midnightblue\") +\n  labs(x = \"Chocolate bar rating\",\n       y = \"Number of bars\")\n\n\n\n\nInitial questions\n\nHave ratings been going up over time?\n\nThe first simple plot we can make to examine this question is a boxplot - we can use ggplot to create a boxplot, mapping review date to the x axis, rating to the y axis, and we include the command group = review_date.\n\ndf %>% \n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_boxplot()\n\n\n\n\nGreat - we can see that the median ranges between 3 and 3.25, increasing in 2010. We can also see that the bottom of the distribution has moved up over time. In other words, there are fewer low scoring chocolate bars over time - potentially indicating an increase in quality of the bars or greater leniency on the part of the reviewers.\nWe can visualize this distribution with the help of another package called ggridges.\nI’ll build up this visualization in stages:\nWe’ll start by mapping the ratings onto the x-axis, the year of the review onto the y-axis (as a factor rather than as a continuous variable).\n\nlibrary(ggridges)\n\ndf %>% \n  ggplot(aes(rating, y = factor(review_date))) +\n  geom_density_ridges()\n\n\n\n\nGreat - we can see that the pattern we saw earlier is clear - as time increases up the y-axis, the share of bars receiving reviews below three decreases.\nNext we can add in a colour scale for the fill:\n\ndf %>% \n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n\n\nThe viridis colour scales are nice for two reasons - they’re discernible to people with most forms of colour blindness and print well if you only use black and white. The magma option gives a nice fade from purple to yellow.\nNext we can move the colourbar in the legend to the bottom and increase it in size, and add some labels.\n\ndf %>%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\nWhat can we learn about the number of ingredients and cocoa percentage?\nSelecting the ingredients column, we see that it has the number of ingredients and each ingredient listed after a dash and separated by commas.\n\ndf %>% \n  select(ingredients)\n\n# A tibble: 2,530 x 1\n   ingredients\n   <chr>      \n 1 3- B,S,C   \n 2 3- B,S,C   \n 3 3- B,S,C   \n 4 3- B,S,C   \n 5 3- B,S,C   \n 6 3- B,S,C   \n 7 3- B,S,C   \n 8 4- B,S,C,L \n 9 4- B,S,C,L \n10 4- B,S,C,L \n# ... with 2,520 more rows\n\n\nThe chocolate bar ingredients are:\n\nstr <- \"B = Beans, S = Sugar, S* = Sweetener other than white cane or beet sugar, C = Cocoa Butter, V = Vanilla, L = Lecithin, Sa = Salt\"\n\nstr <- str %>% \n  as_tibble() %>% \n  separate_rows(value, sep = \",\") %>% \n  separate(value, c(\"key\", \"value\"), \"=\") %>% \n  mutate(across(c(key, value), str_squish))\n\nknitr::kable(str)\n\n\n\nkey\nvalue\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\n\ndf <- df %>% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nWe can use the separate function from dplyr to split the ingredients column into two columns, based on the dash which splits the two components.\nThe separate function takes three inputs: 1. the column which you want to split, 2. a character vector of new column names, and 3. finally the regex which separates the columns.\n\ndf %>% \n  separate(ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"), \n           sep = \"-\") %>% \n  select(n_ingredients, ingredients) %>%\n  mutate(n_ingredients = parse_number(n_ingredients))\n\n# A tibble: 2,530 x 2\n   n_ingredients ingredients                         \n           <dbl> <chr>                               \n 1             3 \" beans,sugar,cocoa_butter\"         \n 2             3 \" beans,sugar,cocoa_butter\"         \n 3             3 \" beans,sugar,cocoa_butter\"         \n 4             3 \" beans,sugar,cocoa_butter\"         \n 5             3 \" beans,sugar,cocoa_butter\"         \n 6             3 \" beans,sugar,cocoa_butter\"         \n 7             3 \" beans,sugar,cocoa_butter\"         \n 8             4 \" beans,sugar,cocoa_butter,lecithin\"\n 9             4 \" beans,sugar,cocoa_butter,lecithin\"\n10             4 \" beans,sugar,cocoa_butter,lecithin\"\n# ... with 2,520 more rows\n\n\nWith the number of ingredients in it’s own column now we can ask what share of the chocolate bars\n\n# jpeg(filename = \"figures/Chocolate_bar_ratings.jpeg\",\n#      height = 6,\n#      width = 8,\n#      units = \"in\",\n#      res = 1000)\n\ndf %>% \n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>% \n  mutate(across(c(n_ingredients, cocoa_percent), parse_number),\n         cocoa_percent = cocoa_percent - cocoa_percent %% 5) %>% \n  count(cocoa_percent, n_ingredients) %>% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format(scale = 1)) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars\")\n\n\n\n# dev.off()\n\nWhat is the correlation between ingredients that are used together?\n\ndf <- df %>%\n  separate(ingredients, into = c(\"n_ingredients\", \"ingredients\"), sep = \"-\") %>%\n  separate_rows(ingredients, sep = \",\") %>%\n  filter(!is.na(ingredients)) %>%\n  pivot_wider(names_from = ingredients, values_from = ingredients) %>%\n  mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nWhat do we know about the countries of origin?\n\ndf %>% \n  add_count(country_of_bean_origin) %>%\n  # only include countries with more than 60 bars\n  filter(n > 60) %>% \n  group_by(country_of_bean_origin) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %>% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\nWhat are some fun variables??\n\ndf %>% \n  select(most_memorable_characteristics, rating) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(across(most_memorable_characteristics, str_squish)) %>% \n  add_count(most_memorable_characteristics) %>% \n  filter(n > 15) %>% \n  group_by(most_memorable_characteristics) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  ungroup() %>% \n  arrange(mean_rating) %>% \n  slice(1:10, 69:78)\n\n# A tibble: 20 x 2\n   most_memorable_characteristics mean_rating\n   <chr>                                <dbl>\n 1 chemical                              2.5 \n 2 medicinal                             2.55\n 3 off notes                             2.58\n 4 burnt                                 2.72\n 5 rubber                                2.73\n 6 pungent                               2.73\n 7 metallic                              2.78\n 8 off                                   2.78\n 9 bitter                                2.79\n10 rubbery                               2.83\n11 cocoa                                 3.39\n12 melon                                 3.40\n13 nuts                                  3.40\n14 raisins                               3.41\n15 honey                                 3.42\n16 dried fruit                           3.44\n17 rich cocoa                            3.44\n18 orange                                3.45\n19 rich                                  3.46\n20 strawberry                            3.46\n\n\nWord model\n\ndf_characteristics <- df %>% \n  select(c(most_memorable_characteristics, rating)) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\ndf_characteristics %>% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 x 2\n   most_memorable_characteristics     n\n   <chr>                          <int>\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# ... with 938 more rows\n\n\n\ndf_characteristics %>% \n  group_by(most_memorable_characteristics) %>% \n  add_count() %>% \n  filter(n > 3) %>% \n  mutate(avg_rating = mean(rating)) %>% \n  ungroup() %>% \n  distinct(most_memorable_characteristics, avg_rating) %>% \n  slice_max(avg_rating, n = 12, with_ties = F) %>% \n    mutate(avg_rating = round(avg_rating, 2)) %>% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds <- vfold_cv(df_characteristics)\n\nglmnet_recipe <- \n  recipe(formula = rating ~ ., data = df_characteristics) %>% \n  step_tokenize(most_memorable_characteristics) %>% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% \n  step_tf(most_memorable_characteristics) %>% \n  step_normalize(all_predictors(), -all_nominal())\n\nglmnet_recipe %>% prep() %>% juice()\n\n# A tibble: 6,839 x 101\n   rating tf_most_memorable_~ tf_most_memorab~ tf_most_memorab~ tf_most_memorab~\n    <dbl>               <dbl>            <dbl>            <dbl>            <dbl>\n 1   3.25             -0.0767          -0.0630          -0.0805          -0.0528\n 2   3.25             -0.0767          -0.0630          -0.0805          -0.0528\n 3   3.25             -0.0767          -0.0630          -0.0805          -0.0528\n 4   3.5              -0.0767          -0.0630          -0.0805          -0.0528\n 5   3.5              -0.0767          -0.0630          -0.0805          -0.0528\n 6   3.5              -0.0767          -0.0630          -0.0805          -0.0528\n 7   3.75             -0.0767          -0.0630          -0.0805          -0.0528\n 8   3.75             -0.0767          -0.0630          -0.0805          -0.0528\n 9   3.75             -0.0767          -0.0630          -0.0805          -0.0528\n10   3                -0.0767          -0.0630          -0.0805          -0.0528\n# ... with 6,829 more rows, and 96 more variables:\n#   tf_most_memorable_characteristics_banana <dbl>,\n#   tf_most_memorable_characteristics_base <dbl>,\n#   tf_most_memorable_characteristics_basic <dbl>,\n#   tf_most_memorable_characteristics_berry <dbl>,\n#   tf_most_memorable_characteristics_bitter <dbl>,\n#   tf_most_memorable_characteristics_black <dbl>, ...\n\n\n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\nglmnet_tune %>% \n  autoplot()\n\n\n\n\n\nglmnet_model_final <- finalize_workflow(glmnet_workflow, glmnet_tune %>% \n  select_best())\n\nfinal_fit <- glmnet_model_final %>% \n  fit(df_characteristics)\n\n\nfinal_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %>%\n  mutate(sign = estimate > 0) %>%\n  group_by(sign) %>%\n  mutate(estimate = abs(estimate)) %>% \n  slice_max(estimate, n = 12) %>%\n  ungroup() %>%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %>% \n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\nRegression examples\n\ndf %>% \n  select(rating)\n\n# A tibble: 2,443 x 1\n   rating\n    <dbl>\n 1   3.25\n 2   3.5 \n 3   3.75\n 4   3   \n 5   3   \n 6   3.25\n 7   3.5 \n 8   3.5 \n 9   3.75\n10   2.75\n# ... with 2,433 more rows\n\nlm(data = df %>% mutate(cocoa_percent = parse_number(cocoa_percent)), rating ~ cocoa_percent) %>%\n  tidy()\n\n# A tibble: 2 x 5\n  term          estimate std.error statistic   p.value\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    3.67      0.120       30.5  1.15e-173\n2 cocoa_percent -0.00641   0.00168     -3.82 1.34e-  4"
  }
]